"""
ì£¼ê°„ ì—…ë°ì´íŠ¸ í†µí•© ìŠ¤í¬ë¦½íŠ¸

í¬ë¡¤ë§ â†’ ì „ì²˜ë¦¬ â†’ ë²¡í„° DB ì—…ë°ì´íŠ¸ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„
í•˜ë‚˜ì˜ ëª…ë ¹ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ì˜ ì‹¤íŒ¨ë¥¼ ê°ì§€í•˜ì—¬
ë¡¤ë°±í•˜ê³ , ìƒì„¸í•œ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
"""

import json
import logging
import os
import shutil
import time
from datetime import datetime
from pathlib import Path

import click
import requests
from dotenv import load_dotenv
from rich.console import Console
from rich.logging import RichHandler
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
from rich.table import Table

load_dotenv()

console = Console()

# ============================================
# ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ê²½ë¡œ
# ============================================
LOGS_DIR = Path("logs")
BACKUP_DIR = Path("backups")
VECTORDB_DIR = Path("confluence_vectordb")
BACKUP_JSON = Path("confluence_backup.json")
PROCESSED_JSON = Path("processed_chunks.json")
SYNC_STATE_FILE = Path("last_sync.json")


def _setup_logger(log_path: Path) -> logging.Logger:
    """íŒŒì¼ + ì½˜ì†” ë¡œê±° ì„¤ì •

    Args:
        log_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ

    Returns:
        ì„¤ì •ëœ Logger ì¸ìŠ¤í„´ìŠ¤
    """
    logger = logging.getLogger("weekly_update")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_path, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    )
    logger.addHandler(file_handler)

    # ì½˜ì†” í•¸ë“¤ëŸ¬ (Rich)
    console_handler = RichHandler(
        console=console,
        show_path=False,
        rich_tracebacks=True,
    )
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    return logger


def _create_backup(logger: logging.Logger) -> dict:
    """í˜„ì¬ ìƒíƒœ ë°±ì—… (ë¡¤ë°±ìš©)

    ë²¡í„° DB, ë™ê¸°í™” ìƒíƒœ, ë°±ì—… JSONì˜ ìŠ¤ëƒ…ìƒ·ì„ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        logger: ë¡œê±°

    Returns:
        ë°±ì—… ê²½ë¡œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    rollback_dir = BACKUP_DIR / f"rollback_{timestamp}"
    rollback_dir.mkdir(parents=True, exist_ok=True)

    backup_info = {"rollback_dir": str(rollback_dir), "files": []}

    # ë™ê¸°í™” ìƒíƒœ ë°±ì—…
    try:
        if SYNC_STATE_FILE.exists():
            dest = rollback_dir / SYNC_STATE_FILE.name
            shutil.copy2(SYNC_STATE_FILE, dest)
            backup_info["files"].append(str(dest))
            logger.debug(f"ë°±ì—…: {SYNC_STATE_FILE} â†’ {dest}")
    except Exception as e:
        logger.error(f"ë™ê¸°í™” ìƒíƒœ ë°±ì—… ì‹¤íŒ¨: {e}")
    
    # ë°±ì—… JSON ë°±ì—…
    if BACKUP_JSON.exists():
        dest = rollback_dir / BACKUP_JSON.name
        shutil.copy2(BACKUP_JSON, dest)
        backup_info["files"].append(str(dest))
        logger.debug(f"ë°±ì—…: {BACKUP_JSON} â†’ {dest}")

    # ë²¡í„° DB ë””ë ‰í† ë¦¬ ë°±ì—… (ì¡´ì¬ ì‹œ)
    if VECTORDB_DIR.exists():
        dest = rollback_dir / "confluence_vectordb"
        shutil.copytree(VECTORDB_DIR, dest)
        backup_info["vectordb_backup"] = str(dest)
        logger.debug(f"ë°±ì—…: {VECTORDB_DIR} â†’ {dest}")

    logger.info(f"ë¡¤ë°± ë°±ì—… ìƒì„± ì™„ë£Œ: {rollback_dir}")
    return backup_info


def _rollback(backup_info: dict, logger: logging.Logger):
    """ì‹¤íŒ¨ ì‹œ ì´ì „ ìƒíƒœë¡œ ë¡¤ë°±

    Args:
        backup_info: _create_backup ë°˜í™˜ê°’
        logger: ë¡œê±°
    """
    logger.warning("ë¡¤ë°±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    rollback_dir = Path(backup_info["rollback_dir"])

    try:
        # ë™ê¸°í™” ìƒíƒœ ë³µì›
        sync_backup = rollback_dir / SYNC_STATE_FILE.name
        if sync_backup.exists():
            shutil.copy2(sync_backup, SYNC_STATE_FILE)
            logger.info(f"ë³µì›: {SYNC_STATE_FILE}")

        # ë°±ì—… JSON ë³µì›
        json_backup = rollback_dir / BACKUP_JSON.name
        if json_backup.exists():
            shutil.copy2(json_backup, BACKUP_JSON)
            logger.info(f"ë³µì›: {BACKUP_JSON}")

        # ë²¡í„° DB ë³µì›
        vectordb_backup = backup_info.get("vectordb_backup")
        if vectordb_backup and Path(vectordb_backup).exists():
            if VECTORDB_DIR.exists():
                shutil.rmtree(VECTORDB_DIR)
            shutil.copytree(vectordb_backup, VECTORDB_DIR)
            logger.info(f"ë³µì›: {VECTORDB_DIR}")

        logger.info("ë¡¤ë°± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: {e}")
        logger.error(f"ìˆ˜ë™ ë³µì› í•„ìš”: {rollback_dir}")


# ============================================
# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ í•¨ìˆ˜
# ============================================

def step_check_environment(logger: logging.Logger) -> bool:
    """1ë‹¨ê³„: í™˜ê²½ í™•ì¸

    .env íŒŒì¼, Ollama ì„œë²„, í•„ìˆ˜ ë””ë ‰í† ë¦¬ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

    Returns:
        í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ ì—¬ë¶€
    """
    errors = []

    # .env íŒŒì¼ í™•ì¸
    if not Path(".env").exists():
        errors.append(".env íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (.env.template ì°¸ê³ )")

    # í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸
    required_vars = ["CONFLUENCE_BASE_URL", "CONFLUENCE_USERNAME", "CONFLUENCE_PASSWORD", "ROOT_PAGE_URL"]
    for var in required_vars:
        if not os.getenv(var):
            errors.append(f"í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •: {var}")

    # Ollama ì„œë²„ í™•ì¸
    ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    try:
        resp = requests.get(f"{ollama_host}/api/tags", timeout=5)
        if resp.status_code == 200:
            logger.info(f"Ollama ì„œë²„ ì •ìƒ: {ollama_host}")
        else:
            errors.append(f"Ollama ì„œë²„ ì‘ë‹µ ì´ìƒ: {resp.status_code}")
    except requests.ConnectionError:
        errors.append(f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ollama_host}")
    except Exception as e:
        errors.append(f"Ollama ì„œë²„ í™•ì¸ ì‹¤íŒ¨: {e}")

    # í•„ìš” ë””ë ‰í† ë¦¬ ìƒì„±
    for d in [LOGS_DIR, BACKUP_DIR, Path("confluence_pages")]:
        d.mkdir(exist_ok=True)

    if errors:
        for err in errors:
            logger.error(err)
        return False

    logger.info("í™˜ê²½ í™•ì¸ ì™„ë£Œ")
    return True


def step_crawl(full: bool, logger: logging.Logger) -> bool:
    """2ë‹¨ê³„: Confluence í¬ë¡¤ë§

    Args:
        full: ì „ì²´ í¬ë¡¤ë§ ì—¬ë¶€
        logger: ë¡œê±°

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    from confluence_crawler import ConfluenceCrawler

    crawl_type = "ì „ì²´" if full else "ì¦ë¶„"
    logger.info(f"{crawl_type} í¬ë¡¤ë§ ì‹œì‘")

    try:
        crawler = ConfluenceCrawler(full_crawl=full)
        crawler.run()

        logger.info(
            f"í¬ë¡¤ë§ ì™„ë£Œ - "
            f"ì¶”ê°€: {crawler.stats['added']}, "
            f"ìˆ˜ì •: {crawler.stats['modified']}, "
            f"ì‚­ì œ: {crawler.stats['deleted']}, "
            f"ê±´ë„ˆëœ€: {crawler.stats['skipped']}"
        )
        return True

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return False


def step_check_changes(logger: logging.Logger) -> bool:
    """3ë‹¨ê³„: ë³€ê²½ì‚¬í•­ í™•ì¸

    í¬ë¡¤ë§ ê²°ê³¼ì— ë³€ê²½ëœ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    Returns:
        ë³€ê²½ì‚¬í•­ ì¡´ì¬ ì—¬ë¶€
    """
    if not BACKUP_JSON.exists():
        logger.warning("ë°±ì—… íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False

    try:
        with open(BACKUP_JSON, "r", encoding="utf-8") as f:
            data = json.load(f)

        page_count = len(data.get("pages", []))

        if page_count == 0:
            logger.info("ë³€ê²½ëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        logger.info(f"ë³€ê²½ëœ í˜ì´ì§€: {page_count}ê°œ")
        return True

    except Exception as e:
        logger.error(f"ë³€ê²½ì‚¬í•­ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def step_preprocess(logger: logging.Logger) -> bool:
    """4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    from preprocess_data import load_backup_data, split_into_chunks, save_processed_chunks

    logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

    try:
        pages = load_backup_data(str(BACKUP_JSON))
        chunks = split_into_chunks(pages)
        save_processed_chunks(chunks, str(PROCESSED_JSON))
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ - {len(pages)}ê°œ í˜ì´ì§€ â†’ {len(chunks)}ê°œ ì²­í¬")
        return True

    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return False


def step_vectorize(full: bool, logger: logging.Logger) -> bool:
    """5ë‹¨ê³„: ë²¡í„° DB ì—…ë°ì´íŠ¸

    Args:
        full: ì „ì²´ ì¬êµ¬ì¶• ì—¬ë¶€
        logger: ë¡œê±°

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    if full:
        from build_vectordb import build_vectordb, verify_vectordb
        logger.info("ë²¡í„° DB ì „ì²´ ì¬êµ¬ì¶• ì‹œì‘")

        try:
            result = build_vectordb(rebuild=True)
            if result:
                verify_vectordb()
                logger.info(f"ë²¡í„° DB ì¬êµ¬ì¶• ì™„ë£Œ - {result['total_vectors']}ê°œ ë²¡í„°")
            return True

        except Exception as e:
            logger.error(f"ë²¡í„° DB ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
    else:
        from update_vectordb import update_process
        logger.info("ë²¡í„° DB ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œì‘")

        try:
            result = update_process()
            if result:
                logger.info(
                    f"ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ - "
                    f"ì‚­ì œ: {result['deleted_vectors']}, "
                    f"ì¶”ê°€: {result['added_vectors']}, "
                    f"ì´: {result['final_total']}"
                )
            return True

        except Exception as e:
            logger.error(f"ë²¡í„° DB ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False


# ============================================
# í†µí•© íŒŒì´í”„ë¼ì¸
# ============================================

STEPS = [
    {"name": "í™˜ê²½ í™•ì¸", "icon": "ğŸ”§"},
    {"name": "í¬ë¡¤ë§", "icon": "ğŸ•·ï¸"},
    {"name": "ë³€ê²½ì‚¬í•­ í™•ì¸", "icon": "ğŸ”"},
    {"name": "ì „ì²˜ë¦¬", "icon": "âš™ï¸"},
    {"name": "ë²¡í„° ì—…ë°ì´íŠ¸", "icon": "ğŸ“¦"},
    {"name": "ì™„ë£Œ", "icon": "âœ…"},
]


@click.command()
@click.option("--full", is_flag=True, default=False, help="ì „ì²´ ì¬êµ¬ì¶• (í¬ë¡¤ë§ + ë²¡í„° DB)")
@click.option("--skip-crawl", is_flag=True, default=False, help="í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°")
@click.option("--skip-vectorize", is_flag=True, default=False, help="ë²¡í„°í™” ê±´ë„ˆë›°ê¸°")
def main(full: bool, skip_crawl: bool, skip_vectorize: bool):
    """Confluence RAG ì£¼ê°„ ì—…ë°ì´íŠ¸

    í¬ë¡¤ë§ â†’ ì „ì²˜ë¦¬ â†’ ë²¡í„° DB ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    \b
    ì‚¬ìš© ì˜ˆì‹œ:
        python weekly_update.py              # ì¦ë¶„ ì—…ë°ì´íŠ¸
        python weekly_update.py --full       # ì „ì²´ ì¬êµ¬ì¶•
        python weekly_update.py --skip-crawl # í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°
    """
    start_time = time.time()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    mode = "ì „ì²´ ì¬êµ¬ì¶•" if full else "ì¦ë¶„ ì—…ë°ì´íŠ¸"

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    LOGS_DIR.mkdir(exist_ok=True)
    log_path = LOGS_DIR / f"update_{timestamp}.log"

    # ë¡œê±° ì„¤ì •
    logger = _setup_logger(log_path)

    # --- í—¤ë” ì¶œë ¥ ---
    console.print()
    console.print(Panel(
        f"[bold]ëª¨ë“œ:[/bold] {mode}\n"
        f"[bold]ì‹œì‘:[/bold] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        f"[bold]ë¡œê·¸:[/bold] {log_path}",
        title="[bold magenta]ğŸš€ Confluence RAG ì£¼ê°„ ì—…ë°ì´íŠ¸[/bold magenta]",
        border_style="magenta",
    ))
    console.print()

    logger.info(f"=== ì£¼ê°„ ì—…ë°ì´íŠ¸ ì‹œì‘ ({mode}) ===")

    # ë¡¤ë°±ìš© ë°±ì—… ìƒì„±
    backup_info = _create_backup(logger)
    current_step = 0

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}[/bold blue]"),
        TimeElapsedColumn(),
        console=console,
    ) as progress:

        task = progress.add_task("ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘...", total=len(STEPS))

        try:
            # --- 1ë‹¨ê³„: í™˜ê²½ í™•ì¸ ---
            current_step = 0
            progress.update(task, description=f"{STEPS[0]['icon']} {STEPS[0]['name']}")

            if not step_check_environment(logger):
                console.print("[red]í™˜ê²½ í™•ì¸ ì‹¤íŒ¨. ì—…ë°ì´íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.[/red]")
                logger.error("í™˜ê²½ í™•ì¸ ì‹¤íŒ¨ë¡œ ì—…ë°ì´íŠ¸ ì¤‘ë‹¨")
                return

            progress.advance(task)

            # --- 2ë‹¨ê³„: í¬ë¡¤ë§ ---
            current_step = 1
            progress.update(task, description=f"{STEPS[1]['icon']} {STEPS[1]['name']}")

            if skip_crawl:
                logger.info("í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸° (--skip-crawl)")
                console.print("[yellow]í¬ë¡¤ë§ ê±´ë„ˆëœ€[/yellow]")
            else:
                if not step_crawl(full, logger):
                    raise RuntimeError("í¬ë¡¤ë§ ì‹¤íŒ¨")

            progress.advance(task)

            # --- 3ë‹¨ê³„: ë³€ê²½ì‚¬í•­ í™•ì¸ ---
            current_step = 2
            progress.update(task, description=f"{STEPS[2]['icon']} {STEPS[2]['name']}")

            if not skip_crawl and not step_check_changes(logger):
                console.print("\n[green]ë³€ê²½ëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.[/green]")
                logger.info("ë³€ê²½ì‚¬í•­ ì—†ìŒ - ì—…ë°ì´íŠ¸ ì¢…ë£Œ")
                progress.update(task, completed=len(STEPS))
                _print_summary(start_time, logger, skipped=True)
                return

            progress.advance(task)

            # --- 4ë‹¨ê³„: ì „ì²˜ë¦¬ ---
            current_step = 3
            progress.update(task, description=f"{STEPS[3]['icon']} {STEPS[3]['name']}")

            if not step_preprocess(logger):
                raise RuntimeError("ì „ì²˜ë¦¬ ì‹¤íŒ¨")

            progress.advance(task)

            # --- 5ë‹¨ê³„: ë²¡í„° ì—…ë°ì´íŠ¸ ---
            current_step = 4
            progress.update(task, description=f"{STEPS[4]['icon']} {STEPS[4]['name']}")

            if skip_vectorize:
                logger.info("ë²¡í„°í™” ê±´ë„ˆë›°ê¸° (--skip-vectorize)")
                console.print("[yellow]ë²¡í„°í™” ê±´ë„ˆëœ€[/yellow]")
            else:
                if not step_vectorize(full, logger):
                    raise RuntimeError("ë²¡í„° DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")

            progress.advance(task)

            # --- 6ë‹¨ê³„: ì™„ë£Œ ---
            progress.update(task, description=f"{STEPS[5]['icon']} {STEPS[5]['name']}")
            progress.advance(task)

            logger.info("=== ì£¼ê°„ ì—…ë°ì´íŠ¸ ì •ìƒ ì™„ë£Œ ===")

        except RuntimeError as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ (ë‹¨ê³„ {current_step + 1}: {STEPS[current_step]['name']}): {e}")
            console.print(f"\n[red]âŒ ì‹¤íŒ¨: {STEPS[current_step]['name']}[/red]")
            _rollback(backup_info, logger)
            _print_summary(start_time, logger, failed_step=STEPS[current_step]["name"])
            return

        except KeyboardInterrupt:
            logger.warning("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            console.print("\n[yellow]ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.[/yellow]")
            _rollback(backup_info, logger)
            return

        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            console.print(f"\n[red]âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}[/red]")
            _rollback(backup_info, logger)
            _print_summary(start_time, logger, failed_step="ì•Œ ìˆ˜ ì—†ìŒ")
            return

    # ê²°ê³¼ ìš”ì•½
    _print_summary(start_time, logger)
    console.print(f"\n[dim]ë¡œê·¸ íŒŒì¼: {log_path}[/dim]\n")


def _print_summary(
    start_time: float,
    logger: logging.Logger,
    skipped: bool = False,
    failed_step: str = None,
):
    """ì—…ë°ì´íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥

    Args:
        start_time: ì‹œì‘ ì‹œê°„
        logger: ë¡œê±°
        skipped: ë³€ê²½ì‚¬í•­ ì—†ì–´ì„œ ê±´ë„ˆë›´ ê²½ìš°
        failed_step: ì‹¤íŒ¨í•œ ë‹¨ê³„ëª… (ì—†ìœ¼ë©´ ì„±ê³µ)
    """
    elapsed = time.time() - start_time
    minutes, seconds = divmod(int(elapsed), 60)

    table = Table(title="ì—…ë°ì´íŠ¸ ê²°ê³¼ ìš”ì•½")
    table.add_column("í•­ëª©", style="cyan")
    table.add_column("ê°’", justify="right", style="bold")

    # ìƒíƒœ
    if failed_step:
        table.add_row("ìƒíƒœ", f"[red]ì‹¤íŒ¨ ({failed_step})[/red]")
    elif skipped:
        table.add_row("ìƒíƒœ", "[yellow]ë³€ê²½ì‚¬í•­ ì—†ìŒ[/yellow]")
    else:
        table.add_row("ìƒíƒœ", "[green]ì„±ê³µ[/green]")

    table.add_row("ì†Œìš” ì‹œê°„", f"{minutes}ë¶„ {seconds}ì´ˆ")

    # í¬ë¡¤ë§ í†µê³„ (ë°±ì—… íŒŒì¼ì—ì„œ)
    if BACKUP_JSON.exists():
        try:
            with open(BACKUP_JSON, "r", encoding="utf-8") as f:
                data = json.load(f)
            table.add_row("í¬ë¡¤ë§ëœ í˜ì´ì§€", str(len(data.get("pages", []))))
        except Exception:
            pass

    # ì²­í¬ í†µê³„
    if PROCESSED_JSON.exists():
        try:
            with open(PROCESSED_JSON, "r", encoding="utf-8") as f:
                data = json.load(f)
            table.add_row("ìƒì„±ëœ ì²­í¬", str(data.get("total_chunks", 0)))
        except Exception:
            pass

    # ë²¡í„° DB í†µê³„
    if VECTORDB_DIR.exists():
        try:
            from chromadb import PersistentClient
            from chromadb.config import Settings
            client = PersistentClient(
                path=str(VECTORDB_DIR),
                settings=Settings(anonymized_telemetry=False),
            )
            collection = client.get_collection(name="confluence_pages")
            table.add_row("ë²¡í„° DB ì´ ë²¡í„°", f"{collection.count():,}")
        except Exception:
            pass

        # DB í¬ê¸°
        db_size = sum(f.stat().st_size for f in VECTORDB_DIR.rglob("*") if f.is_file())
        if db_size >= 1024 * 1024:
            table.add_row("ë²¡í„° DB í¬ê¸°", f"{db_size / (1024 * 1024):.1f} MB")
        else:
            table.add_row("ë²¡í„° DB í¬ê¸°", f"{db_size / 1024:.1f} KB")

    console.print()
    console.print(table)

    # ë¡œê±°ì—ë„ ê¸°ë¡
    status = "ì‹¤íŒ¨" if failed_step else ("ê±´ë„ˆëœ€" if skipped else "ì„±ê³µ")
    logger.info(f"ê²°ê³¼: {status} / ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")


if __name__ == "__main__":
    main()
